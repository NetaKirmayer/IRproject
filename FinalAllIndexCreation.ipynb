{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac36d3a",
      "metadata": {
        "id": "5ac36d3a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Worker_Count",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "cf88b954-f39a-412a-d87e-660833e735b6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
            "cluster-a9c1  GCE       2                                       RUNNING  us-central1-a\r\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc clusters list --region us-central1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf199e6a",
      "metadata": {
        "id": "bf199e6a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Setup",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f56ecd",
      "metadata": {
        "id": "d8f56ecd",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Imports",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "import gc\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a897f2",
      "metadata": {
        "id": "38a897f2",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-jar",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 247882 Nov 21 17:33 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
          ]
        }
      ],
      "source": [
        "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
        "!ls -l /usr/lib/spark/jars/graph*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47900073",
      "metadata": {
        "id": "47900073",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-pyspark-import",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72bed56b",
      "metadata": {
        "id": "72bed56b",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-spark-version",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
        "scrolled": true,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cluster-a9c1-m.c.elegant-zodiac-369314.internal:36411\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f052e7faaf0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980e62a5",
      "metadata": {
        "id": "980e62a5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bucket_name",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Put your bucket name below and make sure you can access it without an error\n",
        "bucket_name = 'irproject2066' \n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths=[]\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "for b in blobs:\n",
        "    if b.name.endswith('.parquet'):\n",
        "        paths.append(full_path+b.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c523e7",
      "metadata": {
        "id": "e4c523e7",
        "scrolled": false,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "parquetFile = spark.read.parquet(*paths)\n",
        "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
        "\n",
        "N_temp = parquetFile.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121fe102",
      "metadata": {
        "id": "121fe102",
        "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inverted_index_gcp.py\r\n"
          ]
        }
      ],
      "source": [
        "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp_final.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c101a8",
      "metadata": {
        "id": "57c101a8",
        "scrolled": true,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp_final.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())\n",
        "\n",
        "sc.addFile(\"/home/dataproc/write_to_memory.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c259c402",
      "metadata": {
        "id": "c259c402",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from inverted_index_gcp_final import InvertedIndex\n",
        "from write_to_memory import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582c3f5e",
      "metadata": {
        "id": "582c3f5e",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Body Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ad8fea",
      "metadata": {
        "id": "f3ad8fea",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-token2bucket",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def word_count(text, id):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  counter = Counter(tokens)\n",
        "\n",
        "  returnList = []\n",
        "  for c in counter:\n",
        "    if c not in all_stopwords:\n",
        "      returnList.append((c, (id,counter[c])))\n",
        "\n",
        "  return returnList\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  return sorted(unsorted_pl, key=lambda x: x[0])\n",
        "\n",
        "def calculate_df(postings):\n",
        "  return postings.map(lambda x: (x[0],len(x[1])))\n",
        "\n",
        "  ####### we can add filtering on rare terms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_postings_and_write(postings, directory):\n",
        "  with_bucket_id = postings.map(lambda w: (token2bucket_id(w[0]) , (w[0] , w[1]))).groupByKey()\n",
        "  buckets_mapped = with_bucket_id.map(lambda w: InvertedIndex.write_a_posting_list(w, bucket_name, directory))\n",
        "  return buckets_mapped"
      ],
      "metadata": {
        "id": "P7Vwit9158_a",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "P7Vwit9158_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c8764e",
      "metadata": {
        "id": "55c8764e",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-index_construction",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# word counts map\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "print(\"done1\")\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "print(\"done2\")\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
        "print(\"done3\")\n",
        "w2df = calculate_df(postings_filtered)\n",
        "print(\"done4\")\n",
        "w2df_dict = w2df.collectAsMap()\n",
        "print(\"done5\")\n",
        "_ = partition_postings_and_write(postings_filtered,'postings_body').collect()\n",
        "print(\"done6\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3296f4",
      "metadata": {
        "id": "ab3296f4",
        "nbgrader": {
          "grade": true,
          "grade_id": "collect-posting",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_body'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d2cfb6",
      "metadata": {
        "id": "a5d2cfb6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "inverted = InvertedIndex()\n",
        "inverted.posting_locs = super_posting_locs\n",
        "inverted.df = w2df_dict\n",
        "inverted.write_index('.', 'body_index')\n",
        "\n",
        "index_src = \"body_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_body/{index_src}'\n",
        "!gsutil cp $index_src $index_dst\n",
        "\n",
        "del super_posting_locs\n",
        "del _\n",
        "del word_counts\n",
        "del postings\n",
        "del w2df\n",
        "del postings_filtered\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating norms for all the body of the ducuments based on tf-idf:"
      ],
      "metadata": {
        "id": "FAUqybwXfDLV"
      },
      "id": "FAUqybwXfDLV"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def createNorm(text):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  counter = Counter(tokens)\n",
        "\n",
        "  tf_list = []\n",
        "  N=N_temp\n",
        "  new_counter = Counter()\n",
        "  for c in counter:\n",
        "    if c not in all_stopwords:\n",
        "      tf_list.append((c,counter[c]))\n",
        "      new_counter[c] = counter[c]\n",
        "  doc_len = 0\n",
        "  for y in new_counter.values():\n",
        "    doc_len += y\n",
        "  pow_list = [math.pow((x/doc_len)*(math.log(N/w2df_dict[c],2)) ,2) for c,x in tf_list]\n",
        "  sumOfTf = 0\n",
        "  for x in pow_list:\n",
        "    sumOfTf += x\n",
        "  return math.sqrt(sumOfTf)\n",
        "\n",
        "docs_norms = doc_text_pairs.map(lambda x: (x[1],createNorm(x[0]))).collectAsMap()\n",
        "\n",
        "write_to_memory(docs_norms,'.','body_TFIDFnorma2')\n",
        "\n",
        "doc_norma_src = \"body_TFIDFnorma2.pkl\"\n",
        "doc_norma_dst = f'gs://{bucket_name}/general_files/{doc_norma_src}'\n",
        "!gsutil cp $doc_norma_src $doc_norma_dst\n",
        "\n",
        "del docs_norms\n",
        "del w2df_dict\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "tmIBLpqxquoy",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "tmIBLpqxquoy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving for each document his leangth:"
      ],
      "metadata": {
        "id": "rKcVv4MQfYq4"
      },
      "id": "rKcVv4MQfYq4"
    },
    {
      "cell_type": "code",
      "source": [
        "def DL_calc(id, text):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    count = 0\n",
        "    for token in tokens:\n",
        "      if token not in all_stopwords:\n",
        "        count += 1\n",
        "    return (id,count)\n",
        "\n",
        "temp_DL=doc_text_pairs.map(lambda x:DL_calc(x[1],x[0])).collectAsMap()\n",
        "write_to_memory(temp_DL,'.','DL_bodyIndex')\n",
        "\n",
        "DL_bodyIndex_src = \"DL_bodyIndex.pkl\"\n",
        "DL_bodyIndex_src_dst = f'gs://{bucket_name}/general_files/{DL_bodyIndex_src}'\n",
        "!gsutil cp $DL_bodyIndex_src $DL_bodyIndex_src_dst\n",
        "\n",
        "del temp_DL\n",
        "del doc_text_pairs\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "4eq3rfKL4Vy2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "4eq3rfKL4Vy2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Title Index"
      ],
      "metadata": {
        "id": "elNtpjoffcvT"
      },
      "id": "elNtpjoffcvT"
    },
    {
      "cell_type": "code",
      "source": [
        "doc_title_pairs = parquetFile.select(\"id\", \"title\").rdd"
      ],
      "metadata": {
        "id": "iNTs3J_ev6UN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "iNTs3J_ev6UN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count_title(text, id):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  counter = Counter(tokens)\n",
        "\n",
        "  returnList = []\n",
        "  for c in counter:\n",
        "    if c not in all_stopwords:\n",
        "      returnList.append((c, (id,counter[c])))\n",
        "\n",
        "  return returnList\n",
        "\n",
        "word_counts_title = doc_title_pairs.flatMap(lambda x: word_count_title(x[0], x[1]))\n",
        "\n",
        "def reduce_word_counts_title(unsorted_pl):\n",
        "  return sorted(unsorted_pl, key=lambda x: x[0])\n",
        "\n",
        "postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts_title)\n",
        "\n",
        "def calculate_df_title(postings):\n",
        "  return postings.map(lambda x: (x[0],len(x[1])))\n",
        "\n",
        "\n",
        "#all the terms df dict for the index\n",
        "w2df_title = calculate_df_title(postings_title)\n",
        "w2df_dict_title = w2df_title.collectAsMap()"
      ],
      "metadata": {
        "id": "6j_vHnJEvyE8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "6j_vHnJEvyE8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posting_locs_list_title = partition_postings_and_write(postings_title,'postings_title').collect()\n",
        "\n",
        "super_posting_locs_title = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_title'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs[k].extend(v)"
      ],
      "metadata": {
        "id": "XmIQnD8MwWpg",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "XmIQnD8MwWpg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create an Inverted Index\n",
        "inverted_title = InvertedIndex()\n",
        "inverted_title.posting_locs = super_posting_locs_title\n",
        "inverted_title.df = w2df_dict_title\n",
        "inverted_title.write_index('.', 'title_index')\n",
        "\n",
        "title_index_src = \"title_index.pkl\"\n",
        "title_index_dst = f'gs://{bucket_name}/postings_title/{title_index_src}'\n",
        "!gsutil cp $title_index_src $title_index_dst\n",
        "\n",
        "del super_posting_locs_title\n",
        "del w2df_dict_title\n",
        "del posting_locs_list_title\n",
        "del postings_title\n",
        "del w2df_title\n",
        "del word_counts_title\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "7jweeoO3wNjA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "7jweeoO3wNjA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating a dictionary mapping between doc_id and the document title:"
      ],
      "metadata": {
        "id": "dEr4w30EfsC3"
      },
      "id": "dEr4w30EfsC3"
    },
    {
      "cell_type": "code",
      "source": [
        "title_pairs = doc_title_pairs.collectAsMap()\n",
        "write_to_memory(title_pairs,'.','title_and_id')\n",
        "\n",
        "title_id_src = \"title_and_id.pkl\"\n",
        "title_id_src_dst = f'gs://{bucket_name}/general_files/{title_id_src}'\n",
        "!gsutil cp $title_id_src $title_id_src_dst\n",
        "\n",
        "del title_pairs\n",
        "del doc_title_pairs\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "MVxqd-hXAHJq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "MVxqd-hXAHJq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anchor Index"
      ],
      "metadata": {
        "id": "glsnTKEifmVs"
      },
      "id": "glsnTKEifmVs"
    },
    {
      "cell_type": "code",
      "source": [
        "doc_anchor_pairs = parquetFile.select(\"id\", \"anchor_text\").rdd"
      ],
      "metadata": {
        "id": "LdnBBgkNyAH0",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "LdnBBgkNyAH0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def helperFunc1(d_id):\n",
        "    list_of_edges = map(lambda w: (s_id, w[1]), d_id)\n",
        "    return list_of_edges\n",
        "\n",
        "# position1 = doc_anchor_pairs.flatMapValues(lambda w: helperFunc1(w[0], w[1])).collect()\n",
        "\n",
        "position1 = doc_anchor_pairs.flatMap(lambda w: w[1])\n",
        "# print(position1)\n",
        "\n",
        "def tuple_creator(doc_id, text):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    counter = Counter(tokens)\n",
        "    return_list = []\n",
        "    for term in tokens:\n",
        "      if term not in all_stopwords:\n",
        "        return_list.append((term, (doc_id,counter[term])))\n",
        "    return return_list\n",
        "\n",
        "position2 = position1.flatMap(lambda x: tuple_creator(x[0],x[1]))\n",
        "position3 = position2.groupByKey()\n",
        "position4 = position3.mapValues(lambda x: list(set(x)))\n",
        "temp_df = position4.map(lambda x: (x[0], len(x[1]))).collectAsMap()\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "\n",
        "posting_locs_list = partition_postings_and_write(position4,'postings_anchor').collect()\n",
        "super_posting_locs = defaultdict(list)\n",
        "\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_anchor'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs[k].extend(v)\n",
        "\n",
        "inverted = InvertedIndex()\n",
        "inverted.posting_locs = super_posting_locs\n",
        "inverted.df = temp_df\n",
        "\n",
        "inverted.write_index('.', 'anchor_index')\n",
        "\n",
        "anchor_index_src = \"anchor_index.pkl\"\n",
        "anchor_index_dst = f'gs://{bucket_name}/postings_anchor/{anchor_index_src}'\n",
        "!gsutil cp $anchor_index_src $anchor_index_dst"
      ],
      "metadata": {
        "id": "LgYF7PohyDIa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "LgYF7PohyDIa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}